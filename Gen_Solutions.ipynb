{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60ba84b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T17:01:57.467965Z",
     "iopub.status.busy": "2025-11-23T17:01:57.467514Z",
     "iopub.status.idle": "2025-11-23T17:01:57.679442Z",
     "shell.execute_reply": "2025-11-23T17:01:57.678871Z"
    },
    "papermill": {
     "duration": 0.216078,
     "end_time": "2025-11-23T17:01:57.680681",
     "exception": false,
     "start_time": "2025-11-23T17:01:57.464603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 23 17:01:57 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA H100 80GB HBM3          Off |   00000000:04:00.0 Off |                    0 |\r\n",
      "| N/A   29C    P0             71W /  700W |       1MiB /  81559MiB |      0%      Default |\r\n",
      "|                                         |                        |             Disabled |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71d2005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T17:02:05.442946Z",
     "iopub.status.busy": "2025-11-23T17:02:05.442807Z",
     "iopub.status.idle": "2025-11-23T17:02:05.446714Z",
     "shell.execute_reply": "2025-11-23T17:02:05.446317Z"
    },
    "papermill": {
     "duration": 0.006975,
     "end_time": "2025-11-23T17:02:05.447286",
     "exception": false,
     "start_time": "2025-11-23T17:02:05.440311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class HFConfig:\n",
    "    user: str = \"tiendung6b\"\n",
    "    generator_dataset_name: str = \"qwen3-4b-instruct-2507-ioi_2024_16k\"\n",
    "    hf_token: str = field(default_factory=lambda: os.environ.get(\"HF_TOKEN\"))\n",
    "\n",
    "    @property\n",
    "    def generator_repo_id(self) -> str:\n",
    "        return f\"{self.user}/{self.generator_dataset_name}\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GenerationConfig:\n",
    "    model_id: str = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "    max_new_tokens: int = 16384\n",
    "    # max_new_tokens: int = 256\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.8\n",
    "    num_solutions: int = 1\n",
    "    seed_base: int = 0\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    @property\n",
    "    def seeds(self) -> list[int]:\n",
    "        return [self.seed_base + i for i in range(self.num_solutions)]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class IOIConfig:\n",
    "    split: str = \"test\"\n",
    "    year: Optional[int] = 2024\n",
    "    root: Path = field(default_factory=lambda: Path(__file__).resolve().parent)\n",
    "\n",
    "    @property\n",
    "    def solutions_dir(self) -> Path:\n",
    "        directory = self.root / \"model_solutions\"\n",
    "        directory.mkdir(parents=True, exist_ok=True)\n",
    "        return directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb382a5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T17:02:05.452346Z",
     "iopub.status.busy": "2025-11-23T17:02:05.452212Z",
     "iopub.status.idle": "2025-11-23T17:02:05.457763Z",
     "shell.execute_reply": "2025-11-23T17:02:05.457386Z"
    },
    "papermill": {
     "duration": 0.008942,
     "end_time": "2025-11-23T17:02:05.458339",
     "exception": false,
     "start_time": "2025-11-23T17:02:05.449397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generator.py\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from config import HFConfig, GenerationConfig, IOIConfig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IOISolutionRecord:\n",
    "    \"\"\"\n",
    "    Một lời giải cụ thể cho 1 subtask IOI với 1 seed.\n",
    "    Các field chính:\n",
    "    - year, day, problem_name, problem_id, target_subtask: metadata bài IOI.\n",
    "    - seed: seed dùng để sinh mẫu này (0..49, ...).\n",
    "    - prompt: toàn bộ prompt text mà Qwen nhận được (ít nhất là problem statement).\n",
    "    - generation: raw text model trả về (bao gồm code, có thể kèm noise).\n",
    "    - code: phần C++ đã tách ra từ generation (bên trong ```cpp ... ```).\n",
    "    - uuid: id duy nhất, dùng để join/trace sau này.\n",
    "    - model_kwargs: lưu seed\n",
    "    - metadata: các info phụ (timestamp, usage, ...).\n",
    "    \"\"\"\n",
    "    year: int\n",
    "    day: int\n",
    "    problem_name: str\n",
    "    problem_id: str\n",
    "    subtask: str\n",
    "    target_subtask: str\n",
    "    seed: int\n",
    "    prompt: str\n",
    "    generation: str\n",
    "    code: str\n",
    "    uuid: str\n",
    "    model_kwargs: Dict[str, Any]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"year\": self.year,\n",
    "            \"day\": self.day,\n",
    "            \"problem_name\": self.problem_name,\n",
    "            \"problem_id\": self.problem_id,\n",
    "            \"subtask\": self.subtask,\n",
    "            \"target_subtask\": self.target_subtask,\n",
    "            \"seed\": self.seed,\n",
    "            \"prompt\": self.prompt,\n",
    "            \"generation\": self.generation,\n",
    "            \"code\": self.code,\n",
    "            \"uuid\": self.uuid,\n",
    "            \"model_kwargs\": self.model_kwargs,\n",
    "            \"metadata\": self.metadata,\n",
    "        }\n",
    "\n",
    "\n",
    "class IOISolutionGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_cfg: HFConfig | None = None,\n",
    "        gen_cfg: GenerationConfig | None = None,\n",
    "        ioi_cfg: IOIConfig | None = None,\n",
    "    ) -> None:\n",
    "        self.hf_cfg = hf_cfg or HFConfig()\n",
    "        self.gen_cfg = gen_cfg or GenerationConfig()\n",
    "        self.ioi_cfg = ioi_cfg or IOIConfig(split=\"test\", year=2024)\n",
    "        self._model: AutoModelForCausalLM | None = None\n",
    "        self._tokenizer: AutoTokenizer | None = None\n",
    "\n",
    "\n",
    "    @property\n",
    "    def model(self) -> AutoModelForCausalLM:\n",
    "        if self._model is None:\n",
    "            print(f\"[IOI] Loading model {self.gen_cfg.model_id} on {self.gen_cfg.device}...\")\n",
    "            self._model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.gen_cfg.model_id,\n",
    "                torch_dtype=\"auto\",\n",
    "                device_map=\"auto\" if self.gen_cfg.device == \"cuda\" else None,\n",
    "            )\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self) -> AutoTokenizer:\n",
    "        if self._tokenizer is None:\n",
    "            self._tokenizer = AutoTokenizer.from_pretrained(self.gen_cfg.model_id)\n",
    "        return self._tokenizer\n",
    "\n",
    "\n",
    "    def _load_ioi_subset(self):\n",
    "        print(f\"[IOI] Loading dataset open-r1/ioi (split={self.ioi_cfg.split})...\")\n",
    "        ds = load_dataset(\"open-r1/ioi\", split=self.ioi_cfg.split)\n",
    "\n",
    "        if self.ioi_cfg.year is not None:\n",
    "            ds = ds.filter(lambda ex: ex[\"year\"] == self.ioi_cfg.year)\n",
    "            print(f\"[IOI] Found {len(ds)} subtasks for IOI {self.ioi_cfg.year}.\")\n",
    "        else:\n",
    "            print(f\"[IOI] Found {len(ds)} subtasks for all years.\")\n",
    "        return ds\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def _build_messages(example: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        system_msg = (\n",
    "            \"You are an expert competitive programmer solving IOI problems. \"\n",
    "            \"You must output a single C++17 solution that compiles with g++ and \"\n",
    "            \"respects the given time and memory limits. \"\n",
    "            \"Return ONLY code in a ```cpp fenced block. \"\n",
    "        )\n",
    "\n",
    "        user_msg = (\n",
    "            f\"{example['problem']}\\n\"\n",
    "            \"Important:\\n\"\n",
    "            \"- Use the required function signature and starter code if provided.\\n\"\n",
    "            \"- Do not add extra main() if the grader expects only functions.\\n\"\n",
    "            \"- Do not read or write files unless explicitly required.\\n\"\n",
    "            \"- Avoid extra debug prints.\\n\"\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_code_from_completion(completion_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Tách phần code C++ từ output model.\n",
    "        - Nếu model trả về dạng ```cpp ... ``` thì lấy phần bên trong.\n",
    "        - Nếu không thấy block ```cpp thì fallback = toàn bộ completion_text.\n",
    "        \"\"\"\n",
    "        code = completion_text\n",
    "        if \"```cpp\" in completion_text:\n",
    "            part = completion_text.split(\"```cpp\", 1)[1]\n",
    "            if \"```\" in part:\n",
    "                part = part.split(\"```\", 1)[0]\n",
    "            code = part\n",
    "        return code.strip()\n",
    "    \n",
    "    def _generate_for_example(\n",
    "        self,\n",
    "        example: Dict[str, Any],\n",
    "        seed: int,\n",
    "    ) -> Tuple[str, str, str, Dict[str, int]]:\n",
    "        messages = self._build_messages(example)\n",
    "\n",
    "        prompt_text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        model_inputs = self.tokenizer(\n",
    "            [prompt_text],\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        prompt_tokens = model_inputs.input_ids.shape[1]\n",
    "        prompt_ids = model_inputs[\"input_ids\"][0]\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        if self.model.device.type == \"cuda\":\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=self.gen_cfg.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=self.gen_cfg.temperature,\n",
    "                top_p=self.gen_cfg.top_p,\n",
    "            )[0]\n",
    "\n",
    "        completion_ids = output_ids[len(prompt_ids) :]\n",
    "        completion_text = self.tokenizer.decode(\n",
    "            completion_ids,\n",
    "            skip_special_tokens=True,\n",
    "        ).strip()\n",
    "\n",
    "        completion_tokens = len(completion_ids)\n",
    "        total_tokens = prompt_tokens + completion_tokens\n",
    "        code = self._extract_code_from_completion(completion_text)\n",
    "        usage = {\n",
    "            \"prompt_tokens\": int(prompt_tokens),\n",
    "            \"completion_tokens\": int(completion_tokens),\n",
    "            \"total_tokens\": int(total_tokens),\n",
    "        }\n",
    "\n",
    "        return prompt_text, completion_text, code, usage\n",
    "\n",
    "\n",
    "    def _solution_filename(self, example: Dict[str, Any], seed: int) -> str:\n",
    "        \"\"\"\n",
    "        Đặt tên file .cpp cho 1 (subtask, seed).\n",
    "        Định dạng:\n",
    "            {year}_{problem_id}_{subtask}_s{seed:02d}_qwen3_4b_instruct_2507.cpp\n",
    "        \"\"\"\n",
    "        year = example[\"year\"]\n",
    "        prob_id = example[\"id\"]\n",
    "        subtask = example[\"subtask\"]\n",
    "        return f\"{year}_{prob_id}_{subtask}_s{seed:02d}_qwen3_4b_instruct_2507.cpp\"\n",
    "\n",
    "    def _load_or_generate_code(\n",
    "        self,\n",
    "        example: Dict[str, Any],\n",
    "        seed: int,\n",
    "    ) -> Tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Nếu đã có file .cpp tương ứng seed này thì load, tránh gọi model lại.\n",
    "        Ngược lại:\n",
    "        - Gọi model sinh code với seed.\n",
    "        - Lưu code ra file .cpp.\n",
    "        - Trả về (prompt_text, generation, code, usage).\n",
    "        \"\"\"\n",
    "        fname = self._solution_filename(example, seed)\n",
    "        fpath = self.ioi_cfg.solutions_dir / fname\n",
    "\n",
    "        if fpath.exists():\n",
    "            with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                code = f.read()\n",
    "            return \"\", code, code, {\n",
    "                \"prompt_tokens\": 0,\n",
    "                \"completion_tokens\": 0,\n",
    "                \"total_tokens\": 0,\n",
    "            }\n",
    "\n",
    "        prompt_text, generation, code, usage = self._generate_for_example(example, seed)\n",
    "        with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(code)\n",
    "        return prompt_text, generation, code, usage\n",
    "\n",
    "\n",
    "    def _build_record(self, example: Dict[str, Any], seed: int) -> IOISolutionRecord:\n",
    "        \"\"\"\n",
    "        Sinh (hoặc load) code cho (example, seed) và đóng gói thành IOISolutionRecord.\n",
    "        \"\"\"\n",
    "        prompt_text, generation, code, usage = self._load_or_generate_code(example, seed)\n",
    "        uid = str(uuid.uuid4())\n",
    "        model_kwargs = {\n",
    "            \"temperature\": self.gen_cfg.temperature,\n",
    "            \"top_p\": self.gen_cfg.top_p,\n",
    "            \"max_new_tokens\": self.gen_cfg.max_new_tokens,\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "\n",
    "        metadata = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"usage\": usage,\n",
    "        }\n",
    "\n",
    "        return IOISolutionRecord(\n",
    "            year=example[\"year\"],\n",
    "            day=example[\"day\"],\n",
    "            problem_name=example[\"name\"],\n",
    "            problem_id=example[\"id\"],\n",
    "            subtask=example[\"subtask\"],\n",
    "            target_subtask=example[\"subtask\"],\n",
    "            seed=seed,\n",
    "            prompt=prompt_text,\n",
    "            generation=generation,\n",
    "            code=code,\n",
    "            uuid=uid,\n",
    "            model_kwargs=model_kwargs,\n",
    "            metadata=metadata,\n",
    "        )\n",
    "\n",
    "    def build_records(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Sinh toàn bộ record cho IOI (theo config, mặc định IOI 2024).\n",
    "        Mỗi (problem, subtask) sẽ có `num_solutions` record, mỗi record một seed.\n",
    "        \"\"\"\n",
    "        ds = self._load_ioi_subset()\n",
    "        records: List[Dict[str, Any]] = []\n",
    "\n",
    "        for ex in tqdm(ds, desc=\"Generating Qwen3 solutions for IOI\"):\n",
    "            for seed in self.gen_cfg.seeds:\n",
    "                record = self._build_record(ex, seed)\n",
    "                records.append(record.to_dict())\n",
    "\n",
    "        print(\n",
    "            f\"[IOI] Prepared {len(records)} solution records \"\n",
    "            f\"({len(ds)} subtasks x {len(self.gen_cfg.seeds)} seeds).\"\n",
    "        )\n",
    "        return records\n",
    "\n",
    "    @staticmethod\n",
    "    def to_dataset(records: List[Dict[str, Any]]) -> Dataset:\n",
    "        return Dataset.from_list(records)\n",
    "\n",
    "    def push_to_hf(self, dataset: Dataset, push: bool | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Push dataset solutions lên HuggingFace Hub nếu push=True.\n",
    "        - Nếu push=None (default), sẽ đọc biến môi trường PUSH_TO_HF.\n",
    "          + PUSH_TO_HF=\"1\" -> push\n",
    "          + ngược lại -> không push.\n",
    "        \"\"\"\n",
    "        if push is None:\n",
    "            push = os.environ.get(\"PUSH_TO_HF\", \"0\") == \"1\"\n",
    "\n",
    "        if not push:\n",
    "            print(\"[IOI] PUSH_TO_HF != 1, không push dataset lên HuggingFace.\")\n",
    "            return\n",
    "\n",
    "        repo_id = self.hf_cfg.generator_repo_id\n",
    "        print(f\"[IOI] Pushing solutions dataset to {repo_id} (split=train)...\")\n",
    "        dataset.push_to_hub(repo_id, split=\"train\")\n",
    "        print(\"[IOI] Done pushing to HuggingFace.\")\n",
    "\n",
    "\n",
    "    def run(self, push: bool | None = None) -> Dataset:\n",
    "        \"\"\"\n",
    "        Hàm high-level:\n",
    "        1) build_records() -> list[dict]\n",
    "        2) to_dataset() -> Dataset\n",
    "        3) push_to_hf() (tuỳ chọn)\n",
    "        Trả về object Dataset, có thể inspect/debug nếu cần.\n",
    "        \"\"\"\n",
    "        records = self.build_records()\n",
    "        dataset = self.to_dataset(records)\n",
    "        self.push_to_hf(dataset, push)\n",
    "        return dataset\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generator = IOISolutionGenerator()\n",
    "    generator.run(push=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964ed28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T17:02:05.463090Z",
     "iopub.status.busy": "2025-11-23T17:02:05.462829Z",
     "iopub.status.idle": "2025-11-23T17:02:05.465824Z",
     "shell.execute_reply": "2025-11-23T17:02:05.465463Z"
    },
    "papermill": {
     "duration": 0.006011,
     "end_time": "2025-11-23T17:02:05.466402",
     "exception": false,
     "start_time": "2025-11-23T17:02:05.460391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing generator.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile generator.sh\n",
    "#!/bin/bash\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"==================================================\"\n",
    "echo \"   SETUP MÔI TRƯỜNG & CHẠY IOI GENERATOR\"\n",
    "echo \"==================================================\"\n",
    "python -m pip install --upgrade pip -q\n",
    "python -m pip install -r requirements.txt\n",
    "\n",
    "export HF_TOKEN=\"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\"\n",
    "echo \"[INFO] Cấu hình Git credential...\"\n",
    "git config --global credential.helper store\n",
    "export PUSH_TO_HF=1\n",
    "\n",
    "echo \"[INFO] Biến môi trường PUSH_TO_HF đã được set = 1\"\n",
    "\n",
    "echo \"==================================================\"\n",
    "echo \"   BẮT ĐẦU QUÁ TRÌNH SINH CODE & UPLOAD\"\n",
    "echo \"==================================================\"\n",
    "\n",
    "if [ -f \"generator.py\" ]; then\n",
    "    python generator.py\n",
    "else\n",
    "    echo \"[ERROR] Không tìm thấy file generator.py. Hãy đảm bảo bạn đã lưu code vào đúng file.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"==================================================\"\n",
    "echo \"   HOÀN TẤT!\"\n",
    "echo \"==================================================\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4a3a3ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T17:02:05.470995Z",
     "iopub.status.busy": "2025-11-23T17:02:05.470865Z",
     "iopub.status.idle": "2025-11-23T17:46:58.394929Z",
     "shell.execute_reply": "2025-11-23T17:46:58.394328Z"
    },
    "papermill": {
     "duration": 2692.927883,
     "end_time": "2025-11-23T17:46:58.396300",
     "exception": false,
     "start_time": "2025-11-23T17:02:05.468417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\r\n",
      "   SETUP MÔI TRƯỜNG & CHẠY IOI GENERATOR\r\n",
      "==================================================\r\n",
      "[INFO] Cấu hình Git credential...\r\n",
      "[INFO] Biến môi trường PUSH_TO_HF đã được set = 1\r\n",
      "==================================================\r\n",
      "   BẮT ĐẦU QUÁ TRÌNH SINH CODE & UPLOAD\r\n",
      "==================================================\r\n",
      "[IOI] Loading dataset open-r1/ioi (split=test)...\r\n",
      "README.md: 6.06kB [00:00, 4.29MB/s]\r\n",
      "data/train-00000-of-00001.parquet: 100%|█████| 769k/769k [00:00<00:00, 1.60MB/s]\r\n",
      "data/test-00000-of-00001.parquet: 100%|███████| 154k/154k [00:00<00:00, 832kB/s]\r\n",
      "Generating train split: 100%|████████| 229/229 [00:00<00:00, 3394.53 examples/s]\r\n",
      "Generating test split: 100%|███████████| 41/41 [00:00<00:00, 3230.27 examples/s]\r\n",
      "Filter: 100%|██████████████████████████| 41/41 [00:00<00:00, 4575.52 examples/s]\r\n",
      "[IOI] Found 41 subtasks for IOI 2024.\r\n",
      "Generating Qwen3 solutions for IOI:   0%|                | 0/41 [00:00<?, ?it/s]\r\n",
      "tokenizer_config.json: 9.38kB [00:00, 53.9MB/s]\r\n",
      "\r\n",
      "vocab.json: 2.78MB [00:00, 126MB/s]\r\n",
      "\r\n",
      "merges.txt: 1.67MB [00:00, 225MB/s]\r\n",
      "\r\n",
      "tokenizer.json:   0%|                               | 0.00/11.4M [00:00<?, ?B/s]\u001b[A\r\n",
      "tokenizer.json: 100%|██████████████████████| 11.4M/11.4M [00:00<00:00, 97.1MB/s]\r\n",
      "[IOI] Loading model Qwen/Qwen3-4B-Instruct-2507 on cuda...\r\n",
      "\r\n",
      "config.json: 100%|█████████████████████████████| 727/727 [00:00<00:00, 7.92MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\r\n",
      "  warnings.warn(\r\n",
      "2025-11-23 17:02:22.881883: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1763917343.005395      55 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1763917343.042526      55 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "\r\n",
      "model.safetensors.index.json: 32.8kB [00:00, 195MB/s]\r\n",
      "\r\n",
      "Fetching 3 files:   0%|                                   | 0/3 [00:00<?, ?it/s]\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:   0%|             | 0.00/3.96G [00:00<?, ?B/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:   0%|             | 0.00/3.99G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "model-00003-of-00003.safetensors:   0%|             | 0.00/99.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "model-00003-of-00003.safetensors:  33%|█▋   | 32.6M/99.6M [00:00<00:00, 112MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:   0%|    | 630k/3.96G [00:00<1:16:01, 867kB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "model-00003-of-00003.safetensors: 100%|█████| 99.6M/99.6M [00:00<00:00, 129MB/s]\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:   0%|    | 880k/3.99G [00:00<1:12:41, 914kB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:   2%|    | 67.7M/3.96G [00:00<00:44, 87.2MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:   2%|    | 67.9M/3.99G [00:01<00:49, 78.7MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:   3%|▏     | 136M/3.96G [00:01<00:31, 122MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  20%|█▏    | 807M/3.96G [00:01<00:03, 924MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:   3%|▏     | 137M/3.99G [00:01<00:34, 113MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:   7%|▍     | 271M/3.99G [00:01<00:16, 222MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:   8%|▌     | 338M/3.99G [00:01<00:13, 261MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  10%|▌     | 405M/3.99G [00:02<00:11, 320MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  25%|█▎   | 1.01G/3.96G [00:02<00:04, 641MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  14%|▊     | 539M/3.99G [00:02<00:09, 374MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  20%|█▏    | 807M/3.99G [00:02<00:04, 686MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  29%|█▍   | 1.14G/3.96G [00:02<00:05, 529MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  24%|█▍    | 941M/3.99G [00:02<00:04, 708MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  34%|█▋   | 1.34G/3.96G [00:02<00:04, 594MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  39%|█▉   | 1.54G/3.96G [00:02<00:03, 715MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  27%|█▎   | 1.08G/3.99G [00:02<00:04, 599MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  44%|██▏  | 1.75G/3.96G [00:03<00:02, 815MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  32%|█▌   | 1.28G/3.99G [00:03<00:03, 747MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  48%|██▍  | 1.88G/3.96G [00:03<00:02, 735MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  56%|██▊  | 2.22G/3.96G [00:03<00:01, 889MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  35%|█▊   | 1.41G/3.99G [00:03<00:04, 535MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  40%|██   | 1.61G/3.99G [00:03<00:03, 663MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  61%|███  | 2.42G/3.96G [00:03<00:01, 923MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  44%|██▏  | 1.75G/3.99G [00:03<00:03, 702MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  64%|███▏ | 2.55G/3.96G [00:03<00:01, 933MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  70%|██▊ | 2.75G/3.96G [00:04<00:01, 1.01GB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  47%|██▎  | 1.88G/3.99G [00:04<00:03, 649MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  51%|██▌  | 2.02G/3.99G [00:04<00:02, 739MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  73%|███▋ | 2.89G/3.96G [00:04<00:01, 807MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  54%|██▋  | 2.15G/3.99G [00:04<00:02, 699MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  76%|███▊ | 3.02G/3.96G [00:04<00:01, 785MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  63%|██▌ | 2.52G/3.99G [00:04<00:01, 1.01GB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  80%|███▉ | 3.16G/3.96G [00:04<00:01, 760MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  85%|████▏| 3.36G/3.96G [00:04<00:00, 925MB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  68%|███▍ | 2.72G/3.99G [00:04<00:01, 983MB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  90%|███▌| 3.56G/3.96G [00:05<00:00, 1.03GB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  75%|██▉ | 2.99G/3.99G [00:05<00:00, 1.18GB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors:  95%|███▊| 3.76G/3.96G [00:05<00:00, 1.20GB/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  82%|███▎| 3.25G/3.99G [00:05<00:00, 1.38GB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "model-00001-of-00003.safetensors: 100%|█████| 3.96G/3.96G [00:05<00:00, 750MB/s]\r\n",
      "\r\n",
      "Fetching 3 files:  33%|█████████                  | 1/3 [00:05<00:10,  5.43s/it]\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors:  88%|███▌| 3.52G/3.99G [00:05<00:00, 1.56GB/s]\u001b[A\u001b[A\u001b[A\r\n",
      "\r\n",
      "\r\n",
      "model-00002-of-00003.safetensors: 100%|█████| 3.99G/3.99G [00:05<00:00, 715MB/s]\r\n",
      "\r\n",
      "Fetching 3 files: 100%|███████████████████████████| 3/3 [00:05<00:00,  1.91s/it]\r\n",
      "\r\n",
      "Loading checkpoint shards:   0%|                          | 0/3 [00:00<?, ?it/s]\u001b[A\r\n",
      "Loading checkpoint shards:  33%|██████            | 1/3 [00:00<00:01,  1.42it/s]\u001b[A\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:01<00:00,  2.06it/s]\r\n",
      "\r\n",
      "generation_config.json: 100%|██████████████████| 238/238 [00:00<00:00, 3.15MB/s]\r\n",
      "Generating Qwen3 solutions for IOI: 100%|███████| 41/41 [44:37<00:00, 65.30s/it]\r\n",
      "[IOI] Prepared 41 solution records (41 subtasks x 1 seeds).\r\n",
      "[IOI] Pushing solutions dataset to tiendung6b/qwen3-4b-instruct-2507-ioi_2024_16k (split=train)...\r\n",
      "Uploading the dataset shards:   0%|                  | 0/1 [00:00<?, ? shards/s]\r\n",
      "Creating parquet from Arrow format: 100%|████████| 1/1 [00:00<00:00, 323.01ba/s]\r\n",
      "Processing Files (0 / 0)      : |                  |  0.00B /  0.00B            \r\n",
      "New Data Upload               : |                  |  0.00B /  0.00B            \u001b[A\r\n",
      "\r\n",
      "                              : 100%|██████████████|  220kB /  220kB            \u001b[A\u001b[A\r\n",
      "\r\n",
      "Processing Files (1 / 1)      : 100%|██████████████|  220kB /  220kB,   ???B/s  \r\n",
      "New Data Upload               : 100%|██████████████|  220kB /  220kB,   ???B/s  \u001b[A\r\n",
      "\r\n",
      "                              : 100%|██████████████|  220kB /  220kB            \u001b[A\u001b[A\r\n",
      "\r\n",
      "Processing Files (1 / 1)      : 100%|██████████████|  220kB /  220kB,  0.00B/s  \r\n",
      "New Data Upload               : 100%|██████████████|  220kB /  220kB,  0.00B/s  \r\n",
      "                              : 100%|██████████████|  220kB /  220kB            \r\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.96 shards/s]\r\n",
      "[IOI] Done pushing to HuggingFace.\r\n",
      "==================================================\r\n",
      "   HOÀN TẤT!\r\n",
      "==================================================\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x generator.sh\n",
    "!./generator.sh"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "datasetId": 8808359,
     "sourceId": 13830725,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2703.706672,
   "end_time": "2025-11-23T17:46:58.617798",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-23T17:01:54.911126",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
